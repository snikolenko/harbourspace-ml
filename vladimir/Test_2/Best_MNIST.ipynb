{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'tmp/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "data = input_data.read_data_sets(DATA_DIR, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_cnn(stride_size):\n",
    "    W_conv1 = weight_variable([stride_size, stride_size, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    \n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "    \n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "    \n",
    "    # Second layer\n",
    "    W_conv2 = weight_variable([stride_size, stride_size, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "    \n",
    "    # Fully connected layer\n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    # keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    \n",
    "    W_fc2 = weight_variable([1024, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "\n",
    "    # y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    return tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_test():\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        for i in range(n_steps):\n",
    "            batch = data.train.next_batch(minibatch_size)\n",
    "            if i % step_to_print == 0:\n",
    "                train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "                print(\"step {}, training accuracy {}\".format(i, train_accuracy))\n",
    "            train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "        # eval test set \n",
    "        # Full doesn't fit in my RAM, so it's splitted into smaller batches\n",
    "        test_acc = 0\n",
    "        size = data.test.images.shape[0]\n",
    "        n_batches = 4\n",
    "        batch_size = size // n_batches\n",
    "        for i in range(n_batches):\n",
    "            test_acc += accuracy.eval(\n",
    "                feed_dict={x: data.test.images[batch_size * i: batch_size * (i + 1)],\n",
    "                           y_: data.test.labels[batch_size * i: batch_size * (i + 1)],\n",
    "                           keep_prob: 1.0})\n",
    "        print(\"test accuracy {}\".format(test_acc / n_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_steps = 4000\n",
    "minibatch_size = 100\n",
    "step_to_print = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_conv = make_cnn(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride size is 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.07999999821186066\n",
      "step 200, training accuracy 0.9399999976158142\n",
      "step 400, training accuracy 0.8999999761581421\n",
      "step 600, training accuracy 0.9800000190734863\n",
      "step 800, training accuracy 0.9200000166893005\n",
      "step 1000, training accuracy 0.9399999976158142\n",
      "step 1200, training accuracy 0.9399999976158142\n",
      "step 1400, training accuracy 0.9800000190734863\n",
      "step 1600, training accuracy 0.9800000190734863\n",
      "step 1800, training accuracy 0.9599999785423279\n",
      "step 2000, training accuracy 0.9800000190734863\n",
      "step 2200, training accuracy 1.0\n",
      "step 2400, training accuracy 0.9399999976158142\n",
      "step 2600, training accuracy 0.9399999976158142\n",
      "step 2800, training accuracy 1.0\n",
      "step 3000, training accuracy 0.9599999785423279\n",
      "step 3200, training accuracy 0.9800000190734863\n",
      "step 3400, training accuracy 0.9599999785423279\n",
      "step 3600, training accuracy 0.9800000190734863\n",
      "step 3800, training accuracy 1.0\n",
      "test accuracy 0.9843999892473221\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        batch = data.train.next_batch(minibatch_size)\n",
    "        if i % step_to_print == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "            print(\"step {}, training accuracy {}\".format(i, train_accuracy))\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    \n",
    "    # eval test set \n",
    "    # Full doesn't fit in my RAM, so it's splitted into smaller batches\n",
    "    test_acc = 0\n",
    "    size = data.test.images.shape[0]\n",
    "    n_batches = 4\n",
    "    batch_size = size // n_batches\n",
    "    for i in range(n_batches):\n",
    "        test_acc += accuracy.eval(\n",
    "            feed_dict={x: data.test.images[batch_size * i: batch_size * (i + 1)],\n",
    "                       y_: data.test.labels[batch_size * i: batch_size * (i + 1)],\n",
    "                       keep_prob: 1.0})\n",
    "    \n",
    "    print(\"test accuracy {}\".format(test_acc / n_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride size is 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_conv = make_cnn(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_steps = 4000\n",
    "minibatch_size = 100\n",
    "step_to_print = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.03999999910593033\n",
      "step 200, training accuracy 0.8999999761581421\n",
      "step 400, training accuracy 0.9300000071525574\n",
      "step 600, training accuracy 0.8999999761581421\n",
      "step 800, training accuracy 0.9800000190734863\n",
      "step 1000, training accuracy 0.9399999976158142\n",
      "step 1200, training accuracy 0.9700000286102295\n",
      "step 1400, training accuracy 0.9800000190734863\n",
      "step 1600, training accuracy 0.9800000190734863\n",
      "step 1800, training accuracy 0.9800000190734863\n",
      "step 2000, training accuracy 0.9800000190734863\n",
      "step 2200, training accuracy 0.9700000286102295\n",
      "step 2400, training accuracy 0.9599999785423279\n",
      "step 2600, training accuracy 0.9900000095367432\n",
      "step 2800, training accuracy 0.9399999976158142\n",
      "step 3000, training accuracy 0.9900000095367432\n",
      "step 3200, training accuracy 1.0\n",
      "step 3400, training accuracy 0.9700000286102295\n",
      "step 3600, training accuracy 0.9900000095367432\n",
      "step 3800, training accuracy 0.9900000095367432\n",
      "test accuracy 0.9864999949932098\n"
     ]
    }
   ],
   "source": [
    "train_and_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
